"""
ç°¡åŒ–å‘é‡è³‡æ–™åº« - ä½¿ç”¨ FAISS é¿å…ç·¨è­¯å•é¡Œ
"""
import os
import pickle
import numpy as np
from typing import List, Dict, Any
import bs4
from sentence_transformers import SentenceTransformer
from langchain_community.document_loaders import WebBaseLoader, TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter


class SimpleVectorStore:
    """ç°¡åŒ–ç‰ˆå‘é‡è³‡æ–™åº« - ä½¿ç”¨ FAISS"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.encoder = SentenceTransformer(model_name)
        self.documents = []  # å„²å­˜åŸå§‹æ–‡æª”
        self.embeddings = None  # å„²å­˜å‘é‡
        self.index = None  # FAISS ç´¢å¼•
        
    def add_documents(self, docs: List[str]):
        """æ·»åŠ æ–‡æª”åˆ°å‘é‡åº«"""
        print(f"ğŸ“„ æ­£åœ¨è™•ç† {len(docs)} å€‹æ–‡æª”...")
        
        # å„²å­˜æ–‡æª”
        self.documents.extend(docs)
        
        # è¨ˆç®—å‘é‡
        print("ğŸ”¢ æ­£åœ¨è¨ˆç®—å‘é‡åµŒå…¥...")
        new_embeddings = self.encoder.encode(docs, show_progress_bar=True)
        
        if self.embeddings is None:
            self.embeddings = new_embeddings
        else:
            self.embeddings = np.vstack([self.embeddings, new_embeddings])
        
        # å»ºç«‹ FAISS ç´¢å¼•
        self._build_index()
        print(f"âœ… å·²æ·»åŠ  {len(docs)} å€‹æ–‡æª”ï¼Œç¸½è¨ˆ {len(self.documents)} å€‹")
    
    def _build_index(self):
        """å»ºç«‹ FAISS ç´¢å¼•"""
        try:
            import faiss
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å…§ç©ç›¸ä¼¼åº¦
            
            # æ­£è¦åŒ–å‘é‡
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings)
            
        except ImportError:
            print("âš ï¸ FAISS æœªå®‰è£ï¼Œä½¿ç”¨ç°¡å–®çš„ numpy æœå°‹")
            self.index = None
    
    def search(self, query: str, k: int = 3) -> List[Dict]:
        """æœå°‹ç›¸ä¼¼æ–‡æª”"""
        if len(self.documents) == 0:
            return []
        
        # è¨ˆç®—æŸ¥è©¢å‘é‡
        query_embedding = self.encoder.encode([query])
        
        if self.index is not None:
            # ä½¿ç”¨ FAISS æœå°‹
            import faiss
            faiss.normalize_L2(query_embedding)
            scores, indices = self.index.search(query_embedding, k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents):
                    results.append({
                        'content': self.documents[idx],
                        'score': float(score)
                    })
            return results
        else:
            # ä½¿ç”¨ numpy è¨ˆç®—ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity
            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            
            # ç²å–å‰ k å€‹æœ€ç›¸ä¼¼çš„æ–‡æª”
            top_indices = np.argsort(similarities)[-k:][::-1]
            
            results = []
            for idx in top_indices:
                results.append({
                    'content': self.documents[idx],
                    'score': float(similarities[idx])
                })
            return results
    
    def save(self, path: str):
        """å„²å­˜å‘é‡åº«"""
        data = {
            'documents': self.documents,
            'embeddings': self.embeddings,
            'model_name': self.model_name
        }
        
        # ä¿®æ­£è·¯å¾‘å•é¡Œ
        dir_path = os.path.dirname(path)
        if dir_path:  # åªæœ‰ç•¶ç›®éŒ„è·¯å¾‘ä¸ç‚ºç©ºæ™‚æ‰å»ºç«‹
            os.makedirs(dir_path, exist_ok=True)
            
        with open(path, 'wb') as f:
            pickle.dump(data, f)
        print(f"ğŸ’¾ å‘é‡åº«å·²å„²å­˜è‡³: {path}")
    
    def load(self, path: str) -> bool:
        """è¼‰å…¥å‘é‡åº«"""
        try:
            with open(path, 'rb') as f:
                data = pickle.load(f)
            
            self.documents = data['documents']
            self.embeddings = data['embeddings']
            self.model_name = data['model_name']
            
            # é‡æ–°è¼‰å…¥æ¨¡å‹
            self.encoder = SentenceTransformer(self.model_name)
            
            # é‡å»ºç´¢å¼•
            if self.embeddings is not None:
                self._build_index()
            
            print(f"ğŸ“‚ å‘é‡åº«å·²è¼‰å…¥: {len(self.documents)} å€‹æ–‡æª”")
            return True
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥å¤±æ•—: {e}")
            return False


class DocumentProcessor:
    """æ–‡æª”è™•ç†å™¨"""
    
    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
    
    def process_web_urls(self, urls: List[str]) -> List[str]:
        """è™•ç†ç¶²é  URL"""
        all_chunks = []
        
        for url in urls:
            try:
                print(f"ğŸŒ è¼‰å…¥ç¶²é : {url}")
                loader = WebBaseLoader(
                    web_paths=[url],
                    bs_kwargs=dict(
                        parse_only=bs4.SoupStrainer(["p", "h1", "h2", "h3", "article"])
                    )
                )
                docs = loader.load()
                
                for doc in docs:
                    chunks = self.text_splitter.split_text(doc.page_content)
                    all_chunks.extend(chunks)
                    
                print(f"  âœ… å·²è™•ç†ï¼Œç²å¾— {len(chunks)} å€‹æ–‡å­—å¡Š")
                
            except Exception as e:
                print(f"  âŒ è™•ç†å¤±æ•—: {e}")
        
        return [chunk for chunk in all_chunks if len(chunk.strip()) > 50]
    
    def process_text_files(self, file_paths: List[str]) -> List[str]:
        """è™•ç†æ–‡å­—æª”æ¡ˆ"""
        all_chunks = []
        
        for file_path in file_paths:
            try:
                print(f"ğŸ“„ è¼‰å…¥æª”æ¡ˆ: {file_path}")
                chunks = []
                
                if file_path.endswith('.pdf'):
                    loader = PyPDFLoader(file_path)
                    docs = loader.load()
                    for doc in docs:
                        chunks = self.text_splitter.split_text(doc.page_content)
                        all_chunks.extend(chunks)
                elif file_path.endswith(('.tsx', '.ts', '.jsx', '.js', '.py', '.html', '.css')):
                    # è™•ç†ç¨‹å¼ç¢¼æª”æ¡ˆï¼Œä½¿ç”¨ç›´æ¥è®€å–æ–¹å¼
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ç‚ºç¨‹å¼ç¢¼æª”æ¡ˆæ·»åŠ ç‰¹æ®Šè™•ç†
                    file_info = f"æª”æ¡ˆè·¯å¾‘: {file_path}\næª”æ¡ˆé¡å‹: {'React TypeScriptçµ„ä»¶' if file_path.endswith('.tsx') else 'ç¨‹å¼ç¢¼æª”æ¡ˆ'}\n\n"
                    content = file_info + content
                    
                    chunks = self.text_splitter.split_text(content)
                    all_chunks.extend(chunks)
                else:
                    # è™•ç†ä¸€èˆ¬æ–‡å­—æª”æ¡ˆ
                    loader = TextLoader(file_path, encoding='utf-8')
                    docs = loader.load()
                    for doc in docs:
                        chunks = self.text_splitter.split_text(doc.page_content)
                        all_chunks.extend(chunks)
                
                print(f"  âœ… å·²è™•ç†ï¼Œç²å¾— {len(chunks)} å€‹æ–‡å­—å¡Š")
                
            except Exception as e:
                print(f"  âŒ è™•ç†å¤±æ•—: {e}")
                # å¦‚æœUTF-8å¤±æ•—ï¼Œå˜—è©¦å…¶ä»–ç·¨ç¢¼
                try:
                    with open(file_path, 'r', encoding='utf-8-sig') as f:
                        content = f.read()
                    file_info = f"æª”æ¡ˆè·¯å¾‘: {file_path}\n\n"
                    content = file_info + content
                    chunks = self.text_splitter.split_text(content)
                    all_chunks.extend(chunks)
                    print(f"  âœ… é‡æ–°è™•ç†æˆåŠŸï¼Œç²å¾— {len(chunks)} å€‹æ–‡å­—å¡Š")
                except Exception as e2:
                    print(f"  âŒ é‡æ–°è™•ç†ä¹Ÿå¤±æ•—: {e2}")
        
        return [chunk for chunk in all_chunks if len(chunk.strip()) > 50]


def create_vectorstore_from_urls(urls: List[str], save_path: str = "vectorstore.pkl"):
    """å¾ URL å»ºç«‹å‘é‡åº«"""
    processor = DocumentProcessor()
    vectorstore = SimpleVectorStore()
    
    # è™•ç†ç¶²é 
    chunks = processor.process_web_urls(urls)
    
    if chunks:
        vectorstore.add_documents(chunks)
        vectorstore.save(save_path)
        return vectorstore
    else:
        print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æ–‡æª”")
        return None


# ç°¡å–®çš„å‘½ä»¤åˆ—å·¥å…·
if __name__ == "__main__":
    print("ğŸ› ï¸ ç°¡åŒ–å‘é‡è³‡æ–™åº«å·¥å…·")
    print("=" * 40)
    
    # åˆå§‹åŒ–
    processor = DocumentProcessor()
    vectorstore = SimpleVectorStore()
    
    while True:
        print("\nè«‹é¸æ“‡æ“ä½œï¼š")
        print("1. è¼‰å…¥ç¶²é æ–‡æª”")
        print("2. è¼‰å…¥æœ¬åœ°æª”æ¡ˆ")
        print("3. è¼‰å…¥æ•´å€‹ç›®éŒ„")
        print("4. æ¸¬è©¦æœå°‹")
        print("0. é€€å‡º")
        
        choice = input("\nè«‹è¼¸å…¥é¸é …: ").strip()
        
        if choice == "0":
            break
            
        elif choice == "1":
            url = input("è«‹è¼¸å…¥ç¶²é  URL: ").strip()
            if url:
                chunks = processor.process_web_urls([url])
                if chunks:
                    vectorstore.add_documents(chunks)
                    vectorstore.save("vectorstore.pkl")
                    print("âœ… å‘é‡åº«å»ºç«‹å®Œæˆï¼")
                    
        elif choice == "2":
            files = input("è«‹è¼¸å…¥æª”æ¡ˆè·¯å¾‘ (ç”¨é€—è™Ÿåˆ†éš”): ").strip()
            if files:
                file_list = [f.strip() for f in files.split(",")]
                chunks = processor.process_text_files(file_list)
                if chunks:
                    vectorstore.add_documents(chunks)
                    vectorstore.save("vectorstore.pkl")
                    print("âœ… å‘é‡åº«å»ºç«‹å®Œæˆï¼")
                    
        elif choice == "3":
            directory = input("è«‹è¼¸å…¥ç›®éŒ„è·¯å¾‘: ").strip()
            if os.path.exists(directory):
                from pathlib import Path
                file_paths = []
                # æ“´å±•æ”¯æ´çš„æª”æ¡ˆæ ¼å¼
                for ext in ['.txt', '.md', '.pdf', '.docx', '.tsx', '.ts', '.jsx', '.js', '.py', '.html', '.css', '.json']:
                    files = list(Path(directory).glob(f"**/*{ext}"))
                    file_paths.extend([str(f) for f in files])
                
                if file_paths:
                    print(f"æ‰¾åˆ° {len(file_paths)} å€‹æª”æ¡ˆ")
                    # é¡¯ç¤ºæ‰¾åˆ°çš„æª”æ¡ˆé¡å‹
                    file_types = {}
                    for fp in file_paths:
                        ext = Path(fp).suffix
                        file_types[ext] = file_types.get(ext, 0) + 1
                    print("æª”æ¡ˆé¡å‹çµ±è¨ˆ:", file_types)
                    
                    chunks = processor.process_text_files(file_paths)
                    if chunks:
                        vectorstore.add_documents(chunks)
                        vectorstore.save("vectorstore.pkl")
                        print("âœ… å‘é‡åº«å»ºç«‹å®Œæˆï¼")
                else:
                    print("âŒ ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ°æ”¯æ´çš„æª”æ¡ˆ")
            else:
                print("âŒ ç›®éŒ„ä¸å­˜åœ¨")
                
        elif choice == "4":
            if vectorstore.load("vectorstore.pkl"):
                while True:
                    query = input("\nè«‹è¼¸å…¥æœå°‹é—œéµå­— (æˆ– 'quit' é€€å‡º): ")
                    if query.lower() == 'quit':
                        break
                    
                    results = vectorstore.search(query, k=3)
                    print(f"\nğŸ” æœå°‹çµæœ ({len(results)} ç­†):")
                    for i, result in enumerate(results, 1):
                        print(f"{i}. [ç›¸ä¼¼åº¦: {result['score']:.3f}]")
                        print(f"   {result['content'][:150]}...\n")
            else:
                print("âŒ è«‹å…ˆå»ºç«‹å‘é‡åº«")
                
        else:
            print("âŒ ç„¡æ•ˆé¸é …")